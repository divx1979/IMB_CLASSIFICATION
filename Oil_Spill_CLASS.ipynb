{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMsX3wOTGOoer3jJ2tjyhN4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/divx1979/IMB_CLASSIFICATION/blob/main/Oil_Spill_CLASS.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Oil Spill Classification\n",
        "## About Dataset\n",
        "The dataset was developed by starting with satellite images of the ocean, some of which contain an oil spill and some that do not.\n",
        "Images were split into sections and processed using computer vision algorithms to provide a vector of features to describe the contents of the image section or patch.\n",
        "The task is, given a vector that describes the contents of a patch of a satellite image, then predicts whether the patch contains an oil spill or not, e.g. from the illegal or accidental dumping of oil in the ocean.\n",
        "\n",
        "There are two classes and the goal is to distinguish between spill and non-spill using the features for a given ocean patch.\n",
        "\n",
        "Non-Spill: negative case, or majority class.\n",
        "Oil Spill: positive case, or minority"
      ],
      "metadata": {
        "id": "kJPzWMCNifmQ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "StrkqbPNiDN2"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from imblearn.pipeline import Pipeline as IMBPipeline\n",
        "from sklearn.model_selection import RepeatedStratifiedKFold\n",
        "from sklearn.metrics import average_precision_score"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "da1 = pd.read_csv('/content/oil_spill.csv')\n",
        "\n",
        "print(da1.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GCbmJopBjDG1",
        "outputId": "4c06a309-d9a5-49d8-ef2e-32d9d7ade564"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   f_1    f_2      f_3     f_4  f_5       f_6    f_7   f_8      f_9  f_10  \\\n",
            "0    1   2558  1506.09  456.63   90   6395000  40.88  7.89  29780.0  0.19   \n",
            "1    2  22325    79.11  841.03  180  55812500  51.11  1.21  61900.0  0.02   \n",
            "2    3    115  1449.85  608.43   88    287500  40.42  7.34   3340.0  0.18   \n",
            "3    4   1201  1562.53  295.65   66   3002500  42.40  7.97  18030.0  0.19   \n",
            "4    5    312   950.27  440.86   37    780000  41.43  7.03   3350.0  0.17   \n",
            "\n",
            "   ...     f_41      f_42     f_43     f_44   f_45  f_46      f_47   f_48  \\\n",
            "0  ...  2850.00   1000.00   763.16   135.46   3.73     0  33243.19  65.74   \n",
            "1  ...  5750.00  11500.00  9593.48  1648.80   0.60     0  51572.04  65.73   \n",
            "2  ...  1400.00    250.00   150.00    45.13   9.33     1  31692.84  65.81   \n",
            "3  ...  6041.52    761.58   453.21   144.97  13.33     1  37696.21  65.67   \n",
            "4  ...  1320.04    710.63   512.54   109.16   2.58     0  29038.17  65.66   \n",
            "\n",
            "   f_49  target  \n",
            "0  7.95       1  \n",
            "1  6.26       0  \n",
            "2  7.84       1  \n",
            "3  8.07       1  \n",
            "4  7.35       0  \n",
            "\n",
            "[5 rows x 50 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(da1.describe())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T1kRtNO8jU6w",
        "outputId": "0e01c92a-92a1-4d25-a2d3-3ceb70eee7e2"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              f_1           f_2          f_3          f_4         f_5  \\\n",
            "count  937.000000    937.000000   937.000000   937.000000  937.000000   \n",
            "mean    81.588047    332.842049   698.707086   870.992209   84.121665   \n",
            "std     64.976730   1931.938570   599.965577   522.799325   45.361771   \n",
            "min      1.000000     10.000000     1.920000     1.000000    0.000000   \n",
            "25%     31.000000     20.000000    85.270000   444.200000   54.000000   \n",
            "50%     64.000000     65.000000   704.370000   761.280000   73.000000   \n",
            "75%    124.000000    132.000000  1223.480000  1260.370000  117.000000   \n",
            "max    352.000000  32389.000000  1893.080000  2724.570000  180.000000   \n",
            "\n",
            "                f_6         f_7         f_8            f_9        f_10  ...  \\\n",
            "count  9.370000e+02  937.000000  937.000000     937.000000  937.000000  ...   \n",
            "mean   7.696964e+05   43.242721    9.127887    3940.712914    0.221003  ...   \n",
            "std    3.831151e+06   12.718404    3.588878    8167.427625    0.090316  ...   \n",
            "min    7.031200e+04   21.240000    0.830000     667.000000    0.020000  ...   \n",
            "25%    1.250000e+05   33.650000    6.750000    1371.000000    0.160000  ...   \n",
            "50%    1.863000e+05   39.970000    8.200000    2090.000000    0.200000  ...   \n",
            "75%    3.304680e+05   52.420000   10.760000    3435.000000    0.260000  ...   \n",
            "max    7.131500e+07   82.640000   24.690000  160740.000000    0.740000  ...   \n",
            "\n",
            "               f_41          f_42         f_43         f_44        f_45  \\\n",
            "count    937.000000    937.000000   937.000000   937.000000  937.000000   \n",
            "mean     933.928677    427.565582   255.435902   106.112519    5.014002   \n",
            "std     1001.681331    715.391648   534.306194   135.617708    5.029151   \n",
            "min        0.000000      0.000000     0.000000     0.000000    0.000000   \n",
            "25%      450.000000    180.000000    90.800000    50.120000    2.370000   \n",
            "50%      685.420000    270.000000   161.650000    73.850000    3.850000   \n",
            "75%     1053.420000    460.980000   265.510000   125.810000    6.320000   \n",
            "max    11949.330000  11500.000000  9593.480000  1748.130000   76.630000   \n",
            "\n",
            "             f_46          f_47        f_48        f_49      target  \n",
            "count  937.000000    937.000000  937.000000  937.000000  937.000000  \n",
            "mean     0.128068   7985.718004   61.694386    8.119723    0.043757  \n",
            "std      0.334344   6854.504915   10.412807    2.908895    0.204662  \n",
            "min      0.000000   2051.500000   35.950000    5.810000    0.000000  \n",
            "25%      0.000000   3760.570000   65.720000    6.340000    0.000000  \n",
            "50%      0.000000   5509.430000   65.930000    7.220000    0.000000  \n",
            "75%      0.000000   9521.930000   66.130000    7.840000    0.000000  \n",
            "max      1.000000  55128.460000   66.450000   15.440000    1.000000  \n",
            "\n",
            "[8 rows x 50 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Split the data Method 1:"
      ],
      "metadata": {
        "id": "PqCoAF4Ljbld"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X = da1.drop('target', axis = 1)\n",
        "y = da1['target']\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 1, stratify = y)"
      ],
      "metadata": {
        "id": "eQocVDDi20cJ"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Build Pipeline\n",
        "\n",
        "def build_pipe(model):\n",
        "  return Pipeline([\n",
        "      ('sc', StandardScaler()),\n",
        "      ('model', model)\n",
        "  ])\n",
        "\n",
        "##Log Reg\n",
        "lr_mod1 = build_pipe(LogisticRegression(solver = 'liblinear', random_state = 42))\n",
        "\n",
        "##Deci Tree\n",
        "dt_mod2 = DecisionTreeClassifier(random_state = 42)\n",
        "\n",
        "##SVM\n",
        "svm_mod3 = build_pipe(SVC(probability = True, random_state = 42))\n",
        "\n",
        "##RF\n",
        "rf_mod4 = RandomForestClassifier(random_state = 42)\n",
        "\n",
        "##GBM\n",
        "gbm_mod5 = GradientBoostingClassifier(random_state = 42)"
      ],
      "metadata": {
        "id": "Jt79R2Fq3LGU"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Spot Check Alg"
      ],
      "metadata": {
        "id": "Uqzko3sw5kUC"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mods = {\n",
        "    'Logistic Regression': lr_mod1,\n",
        "    'Decision Tree': dt_mod2,\n",
        "    'SVM': svm_mod3,\n",
        "    'Random Forest': rf_mod4,\n",
        "    'Gradient Boosting': gbm_mod5\n",
        "}\n",
        "\n",
        "for name, model in mods.items():\n",
        "  model.fit(X_train, y_train)\n",
        "  y_prob = model.predict_proba(X_test)[:, 1]\n",
        "  pr_auc = average_precision_score(y_test, y_prob)\n",
        "  print(pr_auc)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i4fUD28mcrwn",
        "outputId": "75e5dfd0-4fc0-47cb-e52f-39708cddda2a"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.8747957516339869\n",
            "0.4623860182370821\n",
            "0.9166666666666666\n",
            "0.8915178571428571\n",
            "0.8533549783549783\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Handle IMBClass With SMOTE(oversam) And Any 1 Model(e.g. Random Forest)**"
      ],
      "metadata": {
        "id": "9UiILRs3deX_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## SMOTE\n",
        "smote_pipe = IMBPipeline([\n",
        "    ('smote', SMOTE(random_state = 1)),\n",
        "    ('mod', RandomForestClassifier(random_state = 1))\n",
        "])\n",
        "\n",
        "smote_pipe.fit(X_train, y_train)\n",
        "y_pred_smote = smote_pipe.predict_proba(X_test)[:, 1]\n",
        "pr_au1 = average_precision_score(y_test, y_pred_smote)\n",
        "print(pr_au1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LPmE0h-ffah9",
        "outputId": "8688fcdc-2afc-4b40-8be4-f054e028fc4a"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.772165915915916\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Hyperpara Tuning**"
      ],
      "metadata": {
        "id": "ddL3XA4YO3aB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "rf_mod = RandomForestClassifier(random_state=42)  # Base estimator\n",
        "\n",
        "param_grid = { 'n_estimators': [50, 100, 200],\n",
        "               'max_depth': [None, 5, 10]}  # Parameters to test\n",
        "\n",
        "grid_search = GridSearchCV(rf_mod, param_grid, cv=5, scoring='roc_auc')\n",
        "grid_search.fit(X_train, y_train)\n",
        "best_m = grid_search.best_estimator_\n",
        "\n",
        "y_pred_b1 = best_m.predict_proba(X_test)[:, 1]\n",
        "auc_sc_b1 = roc_auc_score(y_test, y_pred_b1)\n",
        "print(auc_sc_b1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tjn2MirfRyim",
        "outputId": "1c1a8a3b-a369-4a63-d048-d37f2ae3d0ec"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.9847222222222223\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Explanation\n",
        "Import Necessary Libraries: First, we import all the required libraries, including those for data manipulation, model building, evaluation, and handling imbalanced data.\n",
        "Load the Dataset: Replace '/path/to/your/oil_spill.csv' with the actual path to your dataset.\n",
        "Split the Data: We divide the dataset into training and testing sets to evaluate model performance.\n",
        "Define Models and Pipelines: For each model, we define a pipeline that includes scaling (when applicable) and the model itself. This is important for algorithms like Logistic Regression and SVM, which are sensitive to the scale of the input features.\n",
        "Spot-Check Algorithms: We fit each model to the training data and evaluate its performance on the test set using the AUROC score, a robust metric for imbalanced datasets.\n",
        "Handle Imbalance with SMOTE: We demonstrate how to use SMOTE (Synthetic Minority Over-sampling Technique) with a pipeline to address class imbalance, using Random Forest as an example model.\n",
        "Hyperparameter Tuning: We perform a grid search to find the optimal hyperparameters for the Random Forest model, then re-evaluate its performance with these optimized settings**"
      ],
      "metadata": {
        "id": "CaCsmgcATtHf"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "bXiYH5dST0I-"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}